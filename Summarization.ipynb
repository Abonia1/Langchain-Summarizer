{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "from langchain import PromptTemplate\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain.callbacks.manager import CallbackManager\n",
    "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"mistral\", callback_manager=CallbackManager([StreamingStdOutCallbackHandler()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded successfully to /Users/aboniasojasingarayar/Documents/Github/Tutorial/data/The-Beginners-Guide-to-Large-Language-Models-1.pdf\n"
     ]
    }
   ],
   "source": [
    "data_folder = Path.cwd() / \"data\"\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_url = \"https://cloudely.com/wp-content/uploads/2024/01/The-Beginners-Guide-to-Large-Language-Models-1.pdf\"\n",
    "\n",
    "pdf_file = str(data_folder / pdf_url.split(\"/\")[-1])\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "# Ensure the response is successful\n",
    "if response.status_code == 200:\n",
    "    # Write the content to a file\n",
    "    with open(pdf_file, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"PDF downloaded successfully to {pdf_file}\")\n",
    "else:\n",
    "    print(f\"Failed to download PDF. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Large Language Models\n",
      "There are several significant types of LLMs, which utilize different\n",
      "architectures and training techniques:\n",
      "Autoregressive Models\n",
      "These were some of the earliest LLMs, including OpenAI's GPT\n",
      "models. They are trained to predict the next word or token in a\n",
      "sequence using previous context. Generating text is done\n",
      "sequentially, one token at a time.\n",
      "Autoencoding Models\n",
      "Examples are BERT, T5, and BART. These models encode the entire\n",
      "input sequence into a latent representation and then decode it back\n",
      "into the original sequence. This allows them to be fine-tuned for\n",
      "various downstream NLP tasks.\n",
      "Encoder-Decoder Models\n",
      "Like GPT-3, these models have separate encoder and decoder\n",
      "components. The encoder ingests the input text while the decoder\n",
      "generates the output text. This architecture provides more flexibility.\n",
      "Sparse Models\n",
      "To reduce computational costs, some models use sparse\n",
      "representations and attention mechanisms. Sparse Transformer is\n",
      "one example of this approach.\n",
      "As LLMs grow in size and complexity, new architectures continue to\n",
      "emerge. But most state-of-the-art models use the Transformer\n",
      "architecture in some form.\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[2].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Stuffing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "  \"\"\"\n",
    "\n",
    "prompt = PromptTemplate(template=prompt_template, input_variables=[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stuff_chain = load_summarize_chain(llm, chain_type=\"stuff\", prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "four_pages = pages[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Large Language Models \\nCSC413 Tutorial 9 \\nYongchao Zhou', metadata={'source': '/Users/aboniasojasingarayar/Documents/Github/Tutorial/data/tut09_llm.pdf', 'page': 0}),\n",
       " Document(page_content='Overview \\n●What are LLMs? \\n●Why LLMs? \\n●Emergent Capabilities \\n○Few-shot In-context Learning \\n○Advanced Prompt Techniques \\n●LLM Training \\n○Architectures \\n○Objectives \\n●LLM Finetuning \\n○Instruction ﬁnetuning \\n○RLHF \\n○Bootstrapping \\n●LLM Risks', metadata={'source': '/Users/aboniasojasingarayar/Documents/Github/Tutorial/data/tut09_llm.pdf', 'page': 1}),\n",
       " Document(page_content='What are Language Models? \\n●Narrow Sense \\n○A probabilistic model that assigns a probability to every ﬁnite sequence (grammatical or not) \\n●Broad Sense \\n○Decoder-only models (GPT-X, OPT, LLaMA, PaLM) \\n○Encoder-only models (BERT, RoBERTa, ELECTRA) \\n○Encoder-decoder models (T5, BART)', metadata={'source': '/Users/aboniasojasingarayar/Documents/Github/Tutorial/data/tut09_llm.pdf', 'page': 2}),\n",
       " Document(page_content='Large Language Models - Billions of Parameters  \\nhttps://huggingface.co/blog/large-language-models', metadata={'source': '/Users/aboniasojasingarayar/Documents/Github/Tutorial/data/tut09_llm.pdf', 'page': 3})]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "four_pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Large Language Models (LLMs) are probabilistic models assigning a probability to every finite sequence, in both narrow and broad senses\n",
      "* Narrow sense: assigns probability to grammatical or not sequences\n",
      "* Broad sense: includes decoder-only, encoder-only, and encoder-decoder models with billions of parameters\n",
      "* Why LLMs?\n",
      "  + Emergent Capabilities\n",
      "    - Few-shot In-context Learning\n",
      "    - Advanced Prompt Techniques\n",
      "* LLM Training: architectures, objectives, finetuning methods\n",
      "* Architectures: transformer models, etc.\n",
      "* Objectives: Maximum Likelihood Estimation (MLE), etc.\n",
      "* Finetuning: Instruction finetuning, RLHF (Reinforcement Learning with Human Feedback), Bootstrapping\n",
      "* Risks associated with LLMs * Large Language Models (LLMs) are probabilistic models assigning a probability to every finite sequence, in both narrow and broad senses\n",
      "* Narrow sense: assigns probability to grammatical or not sequences\n",
      "* Broad sense: includes decoder-only, encoder-only, and encoder-decoder models with billions of parameters\n",
      "* Why LLMs?\n",
      "  + Emergent Capabilities\n",
      "    - Few-shot In-context Learning\n",
      "    - Advanced Prompt Techniques\n",
      "* LLM Training: architectures, objectives, finetuning methods\n",
      "* Architectures: transformer models, etc.\n",
      "* Objectives: Maximum Likelihood Estimation (MLE), etc.\n",
      "* Finetuning: Instruction finetuning, RLHF (Reinforcement Learning with Human Feedback), Bootstrapping\n",
      "* Risks associated with LLMs\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    print(stuff_chain.run(four_pages))\n",
    "except Exception as e:\n",
    "    print(\n",
    "        \"The code failed since it won't be able to run inference on such a huge context and throws this exception: \",\n",
    "        e,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: MapReduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PDF downloaded successfully to /Users/aboniasojasingarayar/Documents/Github/Tutorial/data/The-Beginners-Guide-to-Large-Language-Models-1.pdf\n"
     ]
    }
   ],
   "source": [
    "data_folder = Path.cwd() / \"data\"\n",
    "data_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "pdf_url = \"https://cloudely.com/wp-content/uploads/2024/01/The-Beginners-Guide-to-Large-Language-Models-1.pdf\"\n",
    "\n",
    "pdf_file = str(data_folder / pdf_url.split(\"/\")[-1])\n",
    "response = requests.get(pdf_url)\n",
    "\n",
    "# Ensure the response is successful\n",
    "if response.status_code == 200:\n",
    "    # Write the content to a file\n",
    "    with open(pdf_file, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"PDF downloaded successfully to {pdf_file}\")\n",
    "else:\n",
    "    print(f\"Failed to download PDF. Status code: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Types of Large Language Models\n",
      "There are several significant types of LLMs, which utilize different\n",
      "architectures and training techniques:\n",
      "Autoregressive Models\n",
      "These were some of the earliest LLMs, including OpenAI's GPT\n",
      "models. They are trained to predict the next word or token in a\n",
      "sequence using previous context. Generating text is done\n",
      "sequentially, one token at a time.\n",
      "Autoencoding Models\n",
      "Examples are BERT, T5, and BART. These models encode the entire\n",
      "input sequence into a latent representation and then decode it back\n",
      "into the original sequence. This allows them to be fine-tuned for\n",
      "various downstream NLP tasks.\n",
      "Encoder-Decoder Models\n",
      "Like GPT-3, these models have separate encoder and decoder\n",
      "components. The encoder ingests the input text while the decoder\n",
      "generates the output text. This architecture provides more flexibility.\n",
      "Sparse Models\n",
      "To reduce computational costs, some models use sparse\n",
      "representations and attention mechanisms. Sparse Transformer is\n",
      "one example of this approach.\n",
      "As LLMs grow in size and complexity, new architectures continue to\n",
      "emerge. But most state-of-the-art models use the Transformer\n",
      "architecture in some form.\n"
     ]
    }
   ],
   "source": [
    "pdf_loader = PyPDFLoader(pdf_file)\n",
    "pages = pdf_loader.load_and_split()\n",
    "print(pages[2].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_prompt_template = \"\"\"\n",
    "                      Write a summary of this chunk of text that includes the main points and any important details.\n",
    "                      {text}\n",
    "                      \"\"\"\n",
    "\n",
    "map_prompt = PromptTemplate(template=map_prompt_template, input_variables=[\"text\"])\n",
    "\n",
    "combine_prompt_template = \"\"\"\n",
    "                      Write a concise summary of the following text delimited by triple backquotes.\n",
    "                      Return your response in bullet points which covers the key points of the text.\n",
    "                      ```{text}```\n",
    "                      BULLET POINT SUMMARY:\n",
    "                      \"\"\"\n",
    "\n",
    "combine_prompt = PromptTemplate(\n",
    "    template=combine_prompt_template, input_variables=[\"text\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_reduce_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\",\n",
    "    map_prompt=map_prompt,\n",
    "    combine_prompt=combine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " This text is titled \"The Beginner's Guide to Large Language Models\" from Cloudly.com. The guide aims to provide an introduction and understanding of large language models for beginners. The main points include:\n",
      "\n",
      "1. Definition and explanation of what large language models are, emphasizing their role in natural language processing and machine learning.\n",
      "2. Discussion on the differences between small and large language models, focusing on the scale of training data and the complexity of models.\n",
      "3. Explanation of how large language models are built using deep learning techniques like transformers and recurrent neural networks (RNNs).\n",
      "4. Overview of applications of large language models such as text generation, translation, summarization, question answering, and sentiment analysis.\n",
      "5. Discussion on popular large language models like BERT, GPT-3, T5, and RoBERTa, providing brief descriptions and use cases for each.\n",
      "6. Importance of fine-tuning large language models to adapt them to specific tasks or domains.\n",
      "7. Ethical considerations and potential risks associated with the development and deployment of large language models, such as bias, misinformation, privacy concerns, and security issues. This text introduces Large Language Models (LLMs), which are deep neural networks trained on vast language data using self-supervised objectives. These models have transformer-based architectures and can be trained at massive scales with hundreds of billions of parameters and terabytes of data. Through unsupervised pretraining, LLMs learn rich linguistic representations and contextual relationships between words, enabling them to understand reading comprehension, conversational ability, translation, and more, without specific training. These models can then be fine-tuned for domain-targeted applications like question-answering, summarization, or code generation, or they can retain their wide-ranging abilities while being prompted or constrained for safe use. LLMs have revolutionized natural language capabilities due to their unprecedented scale and pretraining. The text discusses various types of Large Language Models (LLMs) and their key features. These include:\n",
      "\n",
      "1. Autoregressive Models: These were early LLMs, such as OpenAI's GPT models, which predict the next word or token in a sequence based on previous context. Text is generated sequentially, one token at a time.\n",
      "\n",
      "2. Autoencoding Models: Examples include BERT, T5, and BART. They encode the entire input sequence into a latent representation and then decode it back to generate output. This allows fine-tuning for various downstream NLP tasks.\n",
      "\n",
      "3. Encoder-Decoder Models: These models have separate encoder and decoder components. The encoder ingests the input text, while the decoder generates the output text. This architecture offers more flexibility.\n",
      "\n",
      "4. Sparse Models: To reduce computational costs, some models use sparse representations and attention mechanisms, such as Sparse Transformer.\n",
      "\n",
      "As LLMs grow in size and complexity, new architectures continue to emerge, but most state-of-the-art models utilize the Transformer architecture. LLMs, or Large Language Models, utilize the transformer architecture, which was introduced in 2017 and is currently state-of-the-art for language tasks. Transformers consist of encoding-decoding layers with self-attention mechanisms and point-wise feed-forward networks. Self-attention enables parallel processing of relationships between all parts of an input sequence, addressing long-term dependency issues in recurrent models.\n",
      "\n",
      "LLMs are initially trained on vast corpora using self-supervised objectives like masked language modeling (MLM), which requires no human labeling. Pretraining provides broad language intuitions applicable across domains and serves as a solid foundation for downstream optimization through task-specific fine-tuning or continual self-supervision.\n",
      "\n",
      "The advancement of LLMs is characterized by scaling up their architecture, training datasets, and compute resources. Recent developments such as In-Context Learning enable massive models to be queried using prompts for various applications. LLMs (Language Model Machines) have become essential tools in the development of language-focused artificial general intelligence due to their powerful aligned properties. The main components of LLMs include Transformers, Embedding Layers, Context Windows, and Parameters.\n",
      "\n",
      "Transformers are a neural network architecture used in most modern LLMs, based on self-attention mechanisms, which enable the capture of long-range dependencies in text.\n",
      "\n",
      "Embedding Layers convert vocabulary tokens into dense vector representations that encode their meaning.\n",
      "\n",
      "Context Windows are fixed-length text snippets input to the model during training and inference, allowing for modeling of longer dependencies.\n",
      "\n",
      "Parameters store the model's learned knowledge, with large models having trainable weights exceeding hundreds of billions. The evaluation of Language Learning Models (LLMs) poses unique challenges due to their ability to generate open-ended text instead of predictions with clear labels. Evaluation requires nuanced quantitative and qualitative measures. One approach is probing evaluations using specialized diagnostic datasets to test model abilities on specific linguistic phenomena like coreference resolution, causal reasoning, or sentence fusion. Another measure is semantic similarity metrics like BERTScore, which judge how closely generated text aligns with human references based on contextual embedding alignments. Additionally, constitutional AI techniques, such as prompting models to self-evaluate using frameworks like CLIP or Constituent, are used to reflectively assess their abilities and potential harms. The future of LLMs holds significant potential for beneficial change, including bridging communication barriers by facilitating universal translation, equalizing information access globally, and strengthening cross-cultural dialogue and understanding. The text discusses the importance of ensuring beneficial behavior in AI models as capabilities increase, with areas like constitutional AI, self-supervised learning, and whole-model techniques being explored proactively to meet this responsibility. Additionally, large language models are driving transformation across various sectors, leading to new high-paying jobs in fields such as AI safety and oversight, while automating routine tasks for humans to tackle complex challenges. Since 2013, Cloudely, Inc. has offered expertise in Salesforce implementation, staffing solutions, and innovative products like CloudSync and Konfeeg for customer success. The text concludes with an optimistic outlook for the future as AI continues to advance. * \"The Beginner's Guide to Large Language Models\" from Cloudly.com introduces large language models (LLMs), deep neural networks trained on vast language data using self-supervised objectives.\n",
      "* LLMs have transformer-based architectures and can be trained at massive scales with hundreds of billions of parameters and terabytes of data.\n",
      "* Unsupervised pretraining enables LLMs to learn rich linguistic representations and contextual relationships between words, understanding reading comprehension, conversational ability, translation, etc., without specific training.\n",
      "* Fine-tuning adapts LLMs to specific tasks or domains like question-answering, summarization, or code generation.\n",
      "* Key differences between small and large language models: scale of training data, complexity of models.\n",
      "* Applications of large language models include text generation, translation, summarization, question answering, sentiment analysis.\n",
      "* Popular large language models: BERT, GPT-3, T5, RoBERTa; each has brief descriptions and use cases.\n",
      "* Ethical considerations: bias, misinformation, privacy concerns, security issues.\n",
      "* LLMs are transformer-based models, utilizing self-attention mechanisms for parallel processing of relationships between all parts of an input sequence.\n",
      "* Pretraining provides a solid foundation for downstream optimization through task-specific fine-tuning or continual self-supervision.\n",
      "* Evaluation methods include probing evaluations using specialized diagnostic datasets and semantic similarity metrics like BERTScore.\n",
      "* Future potential: bridging communication barriers, equalizing information access globally, strengthening cross-cultural dialogue and understanding.\n",
      "* Importance of ensuring beneficial behavior in AI models as capabilities increase.\n",
      "* LLMs are driving transformation across various sectors, creating new high-paying jobs and automating routine tasks for humans to tackle complex challenges."
     ]
    }
   ],
   "source": [
    "map_reduce_outputs = map_reduce_chain({\"input_documents\": pages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_mp_data = []\n",
    "for doc, out in zip(\n",
    "    map_reduce_outputs[\"input_documents\"], map_reduce_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = Path(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = Path(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_mp_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>concise_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>THE BEGINNER’S GUIDE TO \\nLARGE LANGUAGE MODEL...</td>\n",
       "      <td>This text is titled \"The Beginner's Guide to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you ever feel curious about how human-like ...</td>\n",
       "      <td>This text introduces Large Language Models (L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Types of Large Language Models\\nThere are seve...</td>\n",
       "      <td>The text discusses various types of Large Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>How Do LLMs Really Work?\\nAt their technologic...</td>\n",
       "      <td>LLMs, or Large Language Models, utilize the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>With their powerful yet aligned properties, LL...</td>\n",
       "      <td>LLMs (Language Model Machines) have become es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file_name file_type  page_number  \\\n",
       "0  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            0   \n",
       "1  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            1   \n",
       "2  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            2   \n",
       "3  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            3   \n",
       "4  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            4   \n",
       "\n",
       "                                              chunks  \\\n",
       "0  THE BEGINNER’S GUIDE TO \\nLARGE LANGUAGE MODEL...   \n",
       "1  Do you ever feel curious about how human-like ...   \n",
       "2  Types of Large Language Models\\nThere are seve...   \n",
       "3  How Do LLMs Really Work?\\nAt their technologic...   \n",
       "4  With their powerful yet aligned properties, LL...   \n",
       "\n",
       "                                     concise_summary  \n",
       "0   This text is titled \"The Beginner's Guide to ...  \n",
       "1   This text introduces Large Language Models (L...  \n",
       "2   The text discusses various types of Large Lan...  \n",
       "3   LLMs, or Large Language Models, utilize the t...  \n",
       "4   LLMs (Language Model Machines) have become es...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_mp_summary = pd.DataFrame.from_dict(final_mp_data)\n",
    "pdf_mp_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the dataframe by filename and page_number\n",
    "pdf_mp_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_mp_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "How Do LLMs Really Work?\n",
      "At their technological core, LLMs rely on the transformer\n",
      "architecture first introduced in 2017. Transformers represent the\n",
      "current state-of-the-art for language tasks by tackling long-standing\n",
      "challenges with prior approaches like RNNs.\n",
      "Stacked Self-Attention Layers\n",
      "Transformers consist of stacked encoding-decoding layers\n",
      "containing two sub-layers: multi-head self-attention followed by\n",
      "point-wise feed-forward networks. Self-attention mechanisms relate\n",
      "different positions in a sequence to compute representations for\n",
      "downstream processing.\n",
      "This attention-based approach allows relationships between all\n",
      "parts of an input sequence to be mapped simultaneously in parallel.\n",
      "It also avoids issues with long-term dependencies that recurrent\n",
      "models struggle with.\n",
      "Pretraining Through Self-Supervision\n",
      "LLMs are initially trained on massive corpora using self-supervised\n",
      "objectives that require no human labelling. Chief among these is the\n",
      "masked language modelling (MLM) task - predicting randomly\n",
      "masked tokens based on surrounding context.\n",
      "This pretraining endows models with broad language intuitions\n",
      "applicable across domains. Once learned, these representations\n",
      "serve as highly useful starting points for downstream optimization\n",
      "through task-specific fine-tuning or continual self-supervision.\n",
      "Gradual Progress Through Scale\n",
      "Scaling up all aspects of language models - from their architecture\n",
      "depth and breadth to their training datasets and compute resources\n",
      "- has consistently improved capabilities. Recent paradigms like In-\n",
      "Context Learning allow massive models to be queried through\n",
      "prompts for broad applications.\n",
      "\n",
      "\n",
      " [Simple Summary]\n",
      " LLMs, or Large Language Models, utilize the transformer architecture, which was introduced in 2017 and is currently state-of-the-art for language tasks. Transformers consist of encoding-decoding layers with self-attention mechanisms and point-wise feed-forward networks. Self-attention enables parallel processing of relationships between all parts of an input sequence, addressing long-term dependency issues in recurrent models.\n",
      "\n",
      "LLMs are initially trained on vast corpora using self-supervised objectives like masked language modeling (MLM), which requires no human labeling. Pretraining provides broad language intuitions applicable across domains and serves as a solid foundation for downstream optimization through task-specific fine-tuning or continual self-supervision.\n",
      "\n",
      "The advancement of LLMs is characterized by scaling up their architecture, training datasets, and compute resources. Recent developments such as In-Context Learning enable massive models to be queried using prompts for various applications.\n",
      "\n",
      "\n",
      " [Page number]\n",
      "3\n",
      "\n",
      "\n",
      " [Source: file_name]\n",
      "The-Beginners-Guide-to-Large-Language-Models-1\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_mp_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_mp_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_mp_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_mp_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Refine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_prompt_template = \"\"\"\n",
    "                  Please provide a summary of the following text.\n",
    "                  TEXT: {text}\n",
    "                  SUMMARY:\n",
    "                  \"\"\"\n",
    "\n",
    "question_prompt = PromptTemplate(\n",
    "    template=question_prompt_template, input_variables=[\"text\"]\n",
    ")\n",
    "\n",
    "refine_prompt_template = \"\"\"\n",
    "              Write a concise summary of the following text delimited by triple backquotes.\n",
    "              Return your response in bullet points which covers the key points of the text.\n",
    "              ```{text}```\n",
    "              BULLET POINT SUMMARY:\n",
    "              \"\"\"\n",
    "\n",
    "refine_prompt = PromptTemplate(\n",
    "    template=refine_prompt_template, input_variables=[\"text\"]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "refine_chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"refine\",\n",
    "    question_prompt=question_prompt,\n",
    "    refine_prompt=refine_prompt,\n",
    "    return_intermediate_steps=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: The Beginner's Guide to Large Language Models (Source: WWW.CLOUDELY.COM)\n",
      "\n",
      "Summary:\n",
      "This beginner's guide aims to provide an understanding of large language models, their applications, and the key concepts surrounding them. It begins by explaining what large language models are – AI systems designed to generate human-like text based on input data.\n",
      "\n",
      "The text then delves into the history of language models, starting with early statistical models like Markov Models and Hidden Markov Models, followed by the rise of deep learning models in the 2010s. The guide emphasizes that recent advancements in language models can be attributed to three key factors: massive amounts of data, increased computational power, and sophisticated model architectures.\n",
      "\n",
      "Some popular large language models are introduced, such as BERT (Bidirectional Encoder Representations from Transformers), GPT-3 (Generative Pretrained Transformer 3), and T5 (Text-to-Text Transfer Transformer). These models have shown impressive results in various Natural Language Processing (NLP) tasks, including question answering, text summarization, sentiment analysis, and more.\n",
      "\n",
      "The guide also touches on the limitations of large language models, such as generating misinformation or making incorrect assumptions based on input data. Ethical considerations surrounding their use are addressed, with a call for responsible deployment and ongoing research to improve model performance and mitigate potential risks.\n",
      "\n",
      "To wrap up, the text provides resources for further learning on large language models, including websites, books, and academic papers. It concludes by stating that understanding these advanced AI systems can open doors to new opportunities in research, innovation, and problem-solving across various industries. * Large Language Models (LLMs) are deep neural networks trained on vast language data using self-supervised objectives\n",
      "* LLMs have transformer-based architectures allowing parallel training at large scales\n",
      "* Leading models have hundreds of billions of parameters and terabytes of training data\n",
      "* Unsupervised pretraining allows LLMs to learn rich linguistic representations and contextual relationships between words\n",
      "* LLMs have broad language skills like reading comprehension, conversational ability, translation etc. without task-specific training\n",
      "* LLMs can be fine-tuned on domain-targeted datasets for applications like question-answering, summarization or code generation\n",
      "* Retain wide-ranging abilities while prompted or constrained for safe and beneficial use\n",
      "* LLMs have transformed the landscape of natural language capabilities with their unprecedented scale and pretraining. * Types of Large Language Models (LLMs) include Autoregressive, Autoencoding, Encoder-Decoder, and Sparse models\n",
      "* Autoregressive Models, such as OpenAI's GPT models, are trained to predict next word or token using previous context, generating text sequentially.\n",
      "* Autoencoding Models, including BERT, T5, and BART, encode entire input sequence into a latent representation, then decode back for downstream NLP tasks.\n",
      "* Encoder-Decoder Models have separate encoder and decoder components; the encoder ingests input text, while the decoder generates output text, providing more flexibility.\n",
      "* Sparse Models use sparse representations and attention mechanisms to reduce computational costs, with examples like Sparse Transformer.\n",
      "* Most state-of-the-art LLMs use the Transformer architecture in some form as they grow in size and complexity. * LLMs (Large Language Models) use transformer architecture, which was introduced in 2017 and tackles long-standing language challenges.\n",
      "* Transformers consist of stacked encoding-decoding layers with two sub-layers: multi-head self-attention and point-wise feed-forward networks.\n",
      "* Self-attention mechanisms allow simultaneous mapping of relationships between all parts of an input sequence in parallel, avoiding long-term dependency issues.\n",
      "* LLMs are initially trained on massive corpora using self-supervised objectives like masked language modeling (MLM).\n",
      "* Pretraining endows models with broad language intuitions applicable across domains, and learned representations serve as useful starting points for fine-tuning or continual self-supervision.\n",
      "* Scaling up all aspects of LLMs, including architecture depth/breadth, training datasets, and compute resources, consistently improves capabilities.\n",
      "* Recent paradigms like In-Context Learning allow massive models to be queried through prompts for broad applications. * LLMs (Language Model Large Models) are indispensable tools for language-focused AGI\n",
      "* Key components of LLMs include: Transformers, Embedding Layers, Context Windows, and Parameters\n",
      "* Transformers are neural network architectures based on self-attention mechanisms that capture long-range dependencies in text\n",
      "* Embedding Layers convert vocabulary tokens into dense vector representations that encode their meaning\n",
      "* Context Windows input fixed-length text snippets during training and inference, allowing modeling of longer dependencies\n",
      "* Parameters store the model's learned knowledge, with large models having hundreds of billions of them. * Evaluating Language Models (LLMs) presents unique challenges due to open-ended text production without clear labels\n",
      "* Probing evaluations use specialized diagnostic datasets to test model abilities on specific linguistic phenomena\n",
      "* Semantic similarity metrics, such as BERTScore, assess how closely generated text aligns with human references based on contextual embeddings\n",
      "* Constitutional AI techniques involve models self-evaluating using frameworks like CLIP or Constituent to reflectively assess abilities and potential harms\n",
      "* LLMs have the capacity for significant beneficial change if guided properly\n",
      "* They break down communication barriers by facilitating universal translation, increasing information access and cross-cultural dialogue. * Continued progress in AI depends on developing techniques to ensure beneficial model behavior as capabilities rise.\n",
      "* Areas like constitutional AI, self-supervised learning, and whole-model techniques are proactively addressing this responsibility.\n",
      "* Large language models create new high-paying jobs in fields like AI safety, oversight, and training specialist positions.\n",
      "* They automate routine tasks and free human intellect to tackle complex challenges across every sector.\n",
      "* Societal impact of large language models promises to dwarf today's changes with patience, collaboration, and care.\n",
      "* Cloudely, Inc. has been empowering customer success since 2013 through Salesforce implementation, staffing solutions, and innovative products.\n",
      "* Begin growth journey by contacting Cloudely's experts at salesforce@cloudely.com.\n",
      "* An optimistic future awaits with excitement for embracing the gifts of large language models with wisdom and care."
     ]
    }
   ],
   "source": [
    "refine_outputs = refine_chain({\"input_documents\": pages})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_refine_data = []\n",
    "for doc, out in zip(\n",
    "    refine_outputs[\"input_documents\"], refine_outputs[\"intermediate_steps\"]\n",
    "):\n",
    "    output = {}\n",
    "    output[\"file_name\"] = Path(doc.metadata[\"source\"]).stem\n",
    "    output[\"file_type\"] = Path(doc.metadata[\"source\"]).suffix\n",
    "    output[\"page_number\"] = doc.metadata[\"page\"]\n",
    "    output[\"chunks\"] = doc.page_content\n",
    "    output[\"concise_summary\"] = out\n",
    "    final_refine_data.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>file_type</th>\n",
       "      <th>page_number</th>\n",
       "      <th>chunks</th>\n",
       "      <th>concise_summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>0</td>\n",
       "      <td>THE BEGINNER’S GUIDE TO \\nLARGE LANGUAGE MODEL...</td>\n",
       "      <td>This text is titled \"The Beginner's Guide to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>1</td>\n",
       "      <td>Do you ever feel curious about how human-like ...</td>\n",
       "      <td>This text introduces Large Language Models (L...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>2</td>\n",
       "      <td>Types of Large Language Models\\nThere are seve...</td>\n",
       "      <td>The text discusses various types of Large Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>3</td>\n",
       "      <td>How Do LLMs Really Work?\\nAt their technologic...</td>\n",
       "      <td>LLMs, or Large Language Models, utilize the t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The-Beginners-Guide-to-Large-Language-Models-1</td>\n",
       "      <td>.pdf</td>\n",
       "      <td>4</td>\n",
       "      <td>With their powerful yet aligned properties, LL...</td>\n",
       "      <td>LLMs (Language Model Machines) have become es...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        file_name file_type  page_number  \\\n",
       "0  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            0   \n",
       "1  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            1   \n",
       "2  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            2   \n",
       "3  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            3   \n",
       "4  The-Beginners-Guide-to-Large-Language-Models-1      .pdf            4   \n",
       "\n",
       "                                              chunks  \\\n",
       "0  THE BEGINNER’S GUIDE TO \\nLARGE LANGUAGE MODEL...   \n",
       "1  Do you ever feel curious about how human-like ...   \n",
       "2  Types of Large Language Models\\nThere are seve...   \n",
       "3  How Do LLMs Really Work?\\nAt their technologic...   \n",
       "4  With their powerful yet aligned properties, LL...   \n",
       "\n",
       "                                     concise_summary  \n",
       "0   This text is titled \"The Beginner's Guide to ...  \n",
       "1   This text introduces Large Language Models (L...  \n",
       "2   The text discusses various types of Large Lan...  \n",
       "3   LLMs, or Large Language Models, utilize the t...  \n",
       "4   LLMs (Language Model Machines) have become es...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_refine_summary = pd.DataFrame.from_dict(final_refine_data)\n",
    "pdf_refine_summary = pdf_mp_summary.sort_values(\n",
    "    by=[\"file_name\", \"page_number\"]\n",
    ")  # sorting the datafram by filename and page_number\n",
    "pdf_refine_summary.reset_index(inplace=True, drop=True)\n",
    "pdf_refine_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Context]\n",
      "How Do LLMs Really Work?\n",
      "At their technological core, LLMs rely on the transformer\n",
      "architecture first introduced in 2017. Transformers represent the\n",
      "current state-of-the-art for language tasks by tackling long-standing\n",
      "challenges with prior approaches like RNNs.\n",
      "Stacked Self-Attention Layers\n",
      "Transformers consist of stacked encoding-decoding layers\n",
      "containing two sub-layers: multi-head self-attention followed by\n",
      "point-wise feed-forward networks. Self-attention mechanisms relate\n",
      "different positions in a sequence to compute representations for\n",
      "downstream processing.\n",
      "This attention-based approach allows relationships between all\n",
      "parts of an input sequence to be mapped simultaneously in parallel.\n",
      "It also avoids issues with long-term dependencies that recurrent\n",
      "models struggle with.\n",
      "Pretraining Through Self-Supervision\n",
      "LLMs are initially trained on massive corpora using self-supervised\n",
      "objectives that require no human labelling. Chief among these is the\n",
      "masked language modelling (MLM) task - predicting randomly\n",
      "masked tokens based on surrounding context.\n",
      "This pretraining endows models with broad language intuitions\n",
      "applicable across domains. Once learned, these representations\n",
      "serve as highly useful starting points for downstream optimization\n",
      "through task-specific fine-tuning or continual self-supervision.\n",
      "Gradual Progress Through Scale\n",
      "Scaling up all aspects of language models - from their architecture\n",
      "depth and breadth to their training datasets and compute resources\n",
      "- has consistently improved capabilities. Recent paradigms like In-\n",
      "Context Learning allow massive models to be queried through\n",
      "prompts for broad applications.\n",
      "\n",
      "\n",
      " [Simple Summary]\n",
      " LLMs, or Large Language Models, utilize the transformer architecture, which was introduced in 2017 and is currently state-of-the-art for language tasks. Transformers consist of encoding-decoding layers with self-attention mechanisms and point-wise feed-forward networks. Self-attention enables parallel processing of relationships between all parts of an input sequence, addressing long-term dependency issues in recurrent models.\n",
      "\n",
      "LLMs are initially trained on vast corpora using self-supervised objectives like masked language modeling (MLM), which requires no human labeling. Pretraining provides broad language intuitions applicable across domains and serves as a solid foundation for downstream optimization through task-specific fine-tuning or continual self-supervision.\n",
      "\n",
      "The advancement of LLMs is characterized by scaling up their architecture, training datasets, and compute resources. Recent developments such as In-Context Learning enable massive models to be queried using prompts for various applications.\n",
      "\n",
      "\n",
      " [Page number]\n",
      "3\n",
      "\n",
      "\n",
      " [Source: file_name]\n",
      "The-Beginners-Guide-to-Large-Language-Models-1\n"
     ]
    }
   ],
   "source": [
    "index = 3\n",
    "print(\"[Context]\")\n",
    "print(pdf_refine_summary[\"chunks\"].iloc[index])\n",
    "print(\"\\n\\n [Simple Summary]\")\n",
    "print(pdf_refine_summary[\"concise_summary\"].iloc[index])\n",
    "print(\"\\n\\n [Page number]\")\n",
    "print(pdf_refine_summary[\"page_number\"].iloc[index])\n",
    "print(\"\\n\\n [Source: file_name]\")\n",
    "print(pdf_refine_summary[\"file_name\"].iloc[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
